---
title: "MY474_Summative1"
author: "45868"
date: "WT 2025"
output: html_document
---

# Exercise 2

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE) 
library(haven)
library(glmnet)
library(dplyr)
library(knitr)
library(ggplot2)
library(randomForest)
library(caret)
```

The problem is to answer whether it is possible to identify AI-averse individuals using a bespoke survey without asking about their attitudes to AI technology. To address this, I firstly carefully examined the 2019 OxiS Questionnarie, sample methodology, and data. This 4-part survey includes questions for all, for Internet users, for ex-users and non-users. In the downloaded data, all the "Don't know" and "Refused to answer" responses have been taken as "NA", no difference with the missing values. 

```{r}
# load the data
d <- haven::read_dta("~/downloads/MY474_45868_summative1/oxis2019ukda.dta")

# check the numbers of different respondents
table(d$usenet)

# create a table showing different kinds of respondents and print it
usenet_table <- data.frame(
  Category = c("Users", "Ex-users", "Non-users"),
  Count = as.numeric(table(d$usenet)),
  Percentage = round(prop.table(table(d$usenet)) * 100, 2)
)
print(usenet_table)
```

To build a prediction model, I need to assign the (proxy) target variable and the predictors. In this case, I chose "agai" (*Artificial Intelligence will bring overall positive benefits for society*) and tranformed it into a binary variable (\<3, AI is bad, marked as 1; \>=3, AI is not bad, marked as 0), considering it is directly asking for people's attitudes. Then I deleted those data with NA values in "agai".

```{r, echo=FALSE}
# check the original agai variable
table(d$agai, useNA = "ifany")

# transform agai into a binary variable
d$agai <- ifelse(d$agai < 3, 1, 
                 ifelse(d$agai >= 3 & !is.na(d$agai), 0, NA))

# delete data with agai = NA
d <- d[!is.na(d$agai), ]

# check the new agai
table(d$agai)
```

As for the predictors, the most challenging task is to deal with a large number of NA values in many features due to the survey structure. I firstly preprocessed the data by dropping 110 features with 95% or more missing data (which includes all ex-user-exclusive questions, because there are only 2% ex-users and the questions are highly overlapping with those for others). Then I conducted a univarate analysis to explore the relevance between remaining features and the target variable. From the plot we can see the socioeconomic status, demography, attitudes (of users and non-users) towards general technology and Internet are reletively highly relevant to agai. 

```{r}
# calculate the missing data proportions for every feature
na_proportions <- colMeans(is.na(d)) * 100
print(summary(na_proportions))

# find features with 95% or more missing data
high_na_features <- names(na_proportions[na_proportions > 95])

# delete these features and print the result
d <- d[, !names(d) %in% high_na_features]
cat("\ndeleted", length(high_na_features), "features")
cat("\nremain features：", ncol(d))

# create a function for univariate analysis
univariate_analysis <- function(df, target_col) {
  results <- data.frame(
    feature = character(),
    chi_square = numeric(),
    p_value = numeric(),
    cramer_v = numeric(),
    na_percent = numeric(),
    stringsAsFactors = FALSE
  )
  
  for(col in names(df)[names(df) != target_col]) {
    # calculate the NA proportions
    na_pct <- mean(is.na(df[[col]])) * 100
    
    # create a contingency table
    cont_table <- table(df[[col]], df[[target_col]], useNA = "no")
    
    # do the chi test and check the expected frequency
    chi_test <- suppressWarnings(chisq.test(cont_table))
    expected <- chi_test$expected
    min_expected <- min(expected)
    warning_msg <- if(min_expected < 5) {
      "low expected frequency"
    } else {
      "normal"
    }
    
    tryCatch({
      # calculate Cramer's V
      n <- sum(cont_table)
      min_dim <- min(nrow(cont_table), ncol(cont_table)) - 1
      cramer_v <- sqrt(chi_test$statistic / (n * min_dim))
      
      # save the results
      results <- rbind(results, data.frame(
        feature = col,
        chi_square = chi_test$statistic,
        p_value = chi_test$p.value,
        cramer_v = cramer_v,
        na_percent = na_pct
      ))
    }, error = function(e) {
      cat("跳过", col, ":", e$message, "\n")
    })
  }
  
  # sort by Cramer's V values
  results <- results[order(-results$cramer_v), ]
  return(results)
}

# run the function
results <- univariate_analysis(d, "agai")

# print the results
print("Top 20 relevant features：")
head(results, 20) %>%
  mutate(
    chi_square = round(chi_square, 2),
    p_value = format.pval(p_value, digits = 3),
    cramer_v = round(cramer_v, 3),
    na_percent = round(na_percent, 1)
  )

# plot the results
ggplot(head(results, 20), aes(x = reorder(feature, cramer_v), y = cramer_v)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(x = "Features", y = "Cramer's V",
       title = "Top 20 Features by Association with agai") +
  theme_minimal()
```



```{r, echo = FALSE}
# analyse the NA in different features and recognise the binary features
na_analysis <- data.frame(
  variable = names(d),
  na_count = sapply(d, function(x) sum(is.na(x))),
  na_percent = sapply(d, function(x) mean(is.na(x)) * 100),
  is_binary = sapply(d, function(x) {
    vals <- unique(na.omit(x))
    length(vals) == 2 && all(vals %in% c(0,1))
  })
)

# sort by NA proportion
na_analysis <- na_analysis[order(-na_analysis$na_percent), ]
print("NA analysis：")
print(na_analysis)

# build a function to deal with NA in different situations
# set the threshold based on proportions of different users
process_data <- function(data, threshold = 14.34) { 
  data_processed <- data
  
  for(col in names(data)) {
    if(col == "agai") next
    
    # calculate the NA proportions
    na_pct <- mean(is.na(data[[col]])) * 100
    
    # check if it is a binary variable
    is_binary <- all(na.omit(data[[col]]) %in% c(0,1))
    
    # transform into category variables
    if(!is.factor(data[[col]])) {
      data_processed[[col]] <- as.factor(data[[col]])
    }
    
    if(na_pct > threshold) {
      # for high-NA-proportion variable, create a new category for NA
      levels(data_processed[[col]]) <- c(levels(data_processed[[col]]), "Unknown")
      data_processed[[col]][is.na(data_processed[[col]])] <- "Unknown"
    } else {
      # for low-NA-proportion binary variable, fill according to the probability of 0 and 1
      if(is_binary) {
        prob_1 <- mean(as.numeric(as.character(data[[col]])) == 1, na.rm = TRUE)
        na_idx <- is.na(data_processed[[col]])
        
        if(sum(na_idx) > 0) {
        fills <- rbinom(sum(na_idx), 1, prob_1)
        data_processed[[col]][na_idx] <- factor(fills, levels = c(0,1))
        }
      } else {
        # for low NA proportion non-binary variable, fill with mode 
        mode_val <- names(sort(table(data[[col]], useNA = "no"), decreasing = TRUE))[1]
        data_processed[[col]][is.na(data_processed[[col]])] <- mode_val
      }
    }
  }
  return(data_processed)
}

# run on d
d_processed <- process_data(d)

# check the results
cat("Total number of NAs in original data", sum(sapply(d, function(x) sum(is.na(x)))), "\n")
cat("Total number of NAs after processing", sum(sapply(d_processed, function(x) sum(is.na(x)))), "\n")
```

```{r, echo = FALSE}
# prepare the predictors, train data set and test data set
predictors <- names(d_processed)[!names(d_processed) %in% c("agai")]
set.seed(123)
train_index <- createDataPartition(d_processed$agai, p = 0.7, list = FALSE)
train_data <- d_processed[train_index, ]
test_data <- d_processed[-train_index, ]

# create the matrix
x_train <- model.matrix(~ ., data = train_data[, predictors])[, -1]  # remove the intercept
x_test <- model.matrix(~ ., data = test_data[, predictors])[, -1]
y_train <- train_data$agai
y_test <- test_data$agai

# build logistic regression model with L1 regularization (good for feature selection)
lr_model <- cv.glmnet(x_train, y_train, family = "binomial", alpha = 1)
lr_pred_prob <- predict(lr_model, x_test, s = "lambda.min", type = "response")
lr_pred <- factor(ifelse(lr_pred_prob > 0.5, 1, 0))

# use confusion matrix to assess the model performance
lr_conf_matrix <- confusionMatrix(lr_pred, factor(y_test))

# get the summary of coefficients and print out
best_lambda <- lr_model$lambda.min
final_coef <- coef(lr_model, s = best_lambda)
coef_summary <- data.frame(
  Variable = rownames(final_coef),
  Coefficient = as.vector(final_coef)
)
coef_summary <- coef_summary[coef_summary$Coefficient != 0, ]
coef_summary$Coefficient <- round(coef_summary$Coefficient, 4)
coef_summary <- coef_summary[order(-abs(coef_summary$Coefficient)), ]
print(coef_summary)

# get the summary of the model and print out
cat("Logistic Regression Model Summary \n")
cat("Best lambda:", best_lambda, "\n")
cat("Non-zero coefficients:", nrow(coef_summary) - 1, "\n") 
cat("\nModel performance: \n")
print(lr_conf_matrix)

# calculate AUC and plot the curve
roc_obj <- roc(test_data$agai, as.numeric(lr_pred_prob))
auc_value <- auc(roc_obj)
plot(roc_obj, main = paste("ROC Curve (AUC =", round(auc_value, 3), ")"))
```

```{r, echo = FALSE}
# 3. 随机森林模型
rf_model <- randomForest(factor(agai) ~ ., 
                        data = train_data[, c("agai", predictors)],
                        ntree = 500,
                        importance = TRUE)

# 随机森林预测和评估
rf_pred <- predict(rf_model, test_data)
rf_conf_matrix <- confusionMatrix(rf_pred, factor(test_data$agai))

# 变量重要性
rf_importance <- importance(rf_model)
rf_importance_df <- data.frame(
  Variable = rownames(rf_importance),
  Importance = rf_importance[, "MeanDecreaseGini"]
)
rf_importance_df <- rf_importance_df[order(-rf_importance_df$Importance), ]

 5. 输出结果
cat("\n随机森林模型结果：\n")
print(rf_conf_matrix)
cat("\n前10个最重要的变量（随机森林）：\n")
print(head(rf_importance_df, 10))

par(mfrow = c(1, 2))
varImpPlot(rf_model, n.var = min(20, length(predictors)),
           main = "Random Forest Variable Importance")

```

## Appendix: All code in this assignment

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
# this chunk generates the complete code appendix. 
# eval=FALSE tells R not to run (``evaluate'') the code here (it was already run before).
```
